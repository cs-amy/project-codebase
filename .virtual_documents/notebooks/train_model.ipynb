








# Delete the project-codebase directory if it exists
!rm -rf project-codebase

!git clone https://github.com/cs-amy/project-codebase.git
%cd project-codebase





from google.colab import drive
drive.mount('/content/drive')


# Create symbolic links to the data directory
!ln -s /content/drive/MyDrive/MScProject/data data

print("Symbolic links created successfully!")





!pip install -r requirements.txt

# Specific versions of PyTorch with CUDA support
!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118





import sys
sys.path.append('/content/project-codebase')

print("Python path set up successfully!")





# Library imports
import os
import torch
import yaml
from pathlib import Path
from datetime import datetime
from rich.console import Console
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from torchvision import transforms
from PIL import Image
from typing import Dict, Tuple, Optional, List

# Module imports
from src.models.letter_classifier import get_model
from src.train.trainer import ModelTrainer
from src.data.data_loader import DataLoader

print("Imports successful!")





config = {
    "model": {
        "architecture": "LetterClassifierCNN",
        "input_shape": [28, 28, 1],  # Height, Width, Channels
        "num_classes": 26,  # a-z
        "dropout_rate": 0.5
    },
    "training": {
        "epochs": 100,
        "batch_size": 64,  # Will be automatically adjusted based on available memory
        "learning_rate": 0.001,
        "weight_decay": 0.0001,
        "optimizer": "adam",
        "lr_scheduler": {
            "use": True,
            "type": "reduce_on_plateau",
            "patience": 5,
            "factor": 0.5,
            "min_lr": 0.00001
        },
        "early_stopping": {
            "use": True,
            "patience": 15,
            "min_delta": 0.001
        }
    },
    "data": {
        "regular": {
            "train_dir": "data/characters/regular/train",
            "test_dir": "data/characters/regular/test"
        },
        "obfuscated": {
            "train_dir": "data/characters/obfuscated/train",
            "test_dir": "data/characters/obfuscated/test"
        },
        "image_size": [28, 28],  # Input image size (height, width)
        "validation_split": 0.2,  # Portion of training data to use for validation
        "shuffle": True,
        "augmentation": {
            "use": True,
            "rotation_range": 10,
            "zoom_range": 0.1,
            "width_shift_range": 0.1,
            "height_shift_range": 0.1,
            "brightness_range": [0.8, 1.2],
            "random_noise": 0.01
        }
    },
    "output": {
        "dir": "outputs/letter_classifier",  # Relative to project root
        "save_frequency": 5,  # Save checkpoint every N epochs
        "keep_best": True   # Keep best model based on validation loss
    }
}

print("Model configuration set up successfully!")





# Initialize rich console
console = Console()

console.print("[green]Logging configured successfully![/green]")





# Routine for validating directory structure
def validate_data_directory(data_dir: Path) -> None:
    """
    Validate the data directory structure.

    Args:
        data_dir: Path to the data/characters directory
    """
    required_dirs = [
        "regular/train",
        "regular/test",
        "obfuscated/train",
        "obfuscated/test"
    ]

    missing_dirs = []
    for dir_path in required_dirs:
        full_path = data_dir / dir_path
        if not full_path.exists():
            missing_dirs.append(dir_path)

    if missing_dirs:
        raise FileNotFoundError(
            f"Missing required directories in {data_dir}:\n" +
            "\n".join(f"- {d}" for d in missing_dirs)
        )

    # Log directory structure
    console.print("[green]Data directory structure validation successful![/green]")
    console.print(f"Root: {data_dir}")
    for dir_path in required_dirs:
        full_path = data_dir / dir_path
        num_files = len(list(full_path.glob("**/*.png")))
        console.print(f"- {dir_path}: {num_files} PNG files")


class CharacterDataset(Dataset):
    """Dataset for character images (regular or obfuscated)."""

    # Class-level character mapping
    CHAR_TO_IDX = {chr(97 + i): i for i in range(26)}  # a-z to 0-25
    IDX_TO_CHAR = {i: chr(97 + i) for i in range(26)}  # 0-25 to a-z

    def __init__(
        self,
        data_dir: str | Path,
        image_size: Tuple[int, int] = (28, 28),
        transform: Optional[transforms.Compose] = None,
        is_training: bool = True
    ):
        """
        Initialize the dataset.

        Args:
            data_dir: Directory containing character images (e.g., data/characters/regular/train)
            image_size: Target size for images (height, width)
            transform: Optional additional transformations
            is_training: Whether this is a training dataset
        """
        self.data_dir = Path(data_dir)
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Data directory not found: {self.data_dir}")

        self.image_size = image_size
        self.is_training = is_training

        # Get all character directories (a-z)
        self.char_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])
        if not self.char_dirs:
            raise ValueError(f"No character directories found in {self.data_dir}")

        # Get all image paths and labels
        self.images, self.labels = self._load_dataset()

        # Set up transformations
        self.transform = transform if transform is not None else self._get_default_transforms()

        # Log dataset statistics
        console.print(f"[green]Loaded {len(self.images)} images from {self.data_dir}[/green]")
        char_counts = {char: sum(1 for l in self.labels if l == idx)
                       for char, idx in self.CHAR_TO_IDX.items()}
        console.print("[green]Character distribution:[/green]")
        for char, count in char_counts.items():
            console.print(f"- {char}: {count} images")

    def _load_dataset(self) -> Tuple[List[Path], List[int]]:
        """Load all image paths and their corresponding labels."""
        images, labels = [], []

        for char_dir in self.char_dirs:
            char = char_dir.name.lower()
            if char not in self.CHAR_TO_IDX:
                console.print(f"[yellow]Skipping unknown character directory: {char}[/yellow]")
                continue

            label = self.CHAR_TO_IDX[char]

            # Get all PNG images in this directory
            char_images = list(char_dir.glob("*.png"))
            images.extend(char_images)
            labels.extend([label] * len(char_images))

        return images, labels

    def _get_default_transforms(self) -> transforms.Compose:
        """Get default transformation pipeline."""
        transform_list = [
            transforms.Resize(self.image_size),
            transforms.Grayscale(1),
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ]

        if self.is_training:
            transform_list.insert(1, transforms.RandomRotation(10))
            transform_list.insert(2, transforms.RandomAffine(
                degrees=0,
                translate=(0.1, 0.1),
                scale=(0.9, 1.1)
            ))

        return transforms.Compose(transform_list)

    def __len__(self) -> int:
        return len(self.images)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        """Get a single sample."""
        image_path = self.images[idx]
        label = self.labels[idx]

        # Load and convert image
        try:
            image = Image.open(image_path).convert('L')  # Convert to grayscale
            image = self.transform(image)
            return image, label
        except Exception as e:
            console.print(f"[red]Error loading image {image_path}: {e}[/red]")
            # Return a blank image and the label if there's an error
            return torch.zeros((1, *self.image_size)), label

    @classmethod
    def get_char_mapping(cls) -> Tuple[Dict[str, int], Dict[int, str]]:
        """
        Get the character to index and index to character mappings.

        Returns:
            Tuple of (char_to_idx, idx_to_char) dictionaries
        """
        return cls.CHAR_TO_IDX, cls.IDX_TO_CHAR


def get_data_loaders(
    data_dir: str | Path,
    batch_size: int = 32,
    image_size: Tuple[int, int] = (28, 28),
    num_workers: int = 4,
    augment: bool = True
) -> Dict[str, DataLoader]:
    """
    Create data loaders for training, validation, and testing.

    Args:
        data_dir: Path to the data/characters directory (e.g., data/characters)
        batch_size: Batch size for training
        image_size: Target size for images
        num_workers: Number of worker processes for data loading
        augment: Whether to use data augmentation (applied in training mode)

    Returns:
        Dictionary containing train, val, and test data loaders.
    """
    data_dir = Path(data_dir)

    # Validate directory structure
    validate_data_directory(data_dir)

    # Load training datasets for both regular and obfuscated characters
    train_regular = CharacterDataset(
        data_dir / "regular" / "train",
        image_size=image_size,
        is_training=True
    )
    train_obfuscated = CharacterDataset(
        data_dir / "obfuscated" / "train",
        image_size=image_size,
        is_training=True
    )

    # Load test datasets for both regular and obfuscated characters
    test_regular = CharacterDataset(
        data_dir / "regular" / "test",
        image_size=image_size,
        is_training=False
    )
    test_obfuscated = CharacterDataset(
        data_dir / "obfuscated" / "test",
        image_size=image_size,
        is_training=False
    )

    # Combine datasets for training and testing
    train_dataset = ConcatDataset([train_regular, train_obfuscated])
    test_dataset = ConcatDataset([test_regular, test_obfuscated])

    # Create train/validation split (80/20) on the training dataset
    train_length = int(0.8 * len(train_dataset))
    val_length = len(train_dataset) - train_length
    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_length, val_length])

    # Log dataset statistics
    console.print("[green]Combined dataset statistics:[/green]")
    console.print(f"- Training set: {len(train_subset)} images")
    console.print(f"- Validation set: {len(val_subset)} images")
    console.print(f"- Test set: {len(test_dataset)} images")

    # Create data loaders for each split
    train_loader = DataLoader(
        train_subset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_subset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    return {
        "train": train_loader,
        "val": val_loader,
        "test": test_loader
    }





def setup_device():
    """
    Set up the training device (GPU/CPU) based on availability.
    Uses MPS for Apple Silicon, CUDA for NVIDIA GPUs, or falls back to CPU.
    """
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        console.print("[green]GPU available: Using Metal Performance Shaders (MPS)[/green]")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        console.print("[green]GPU available: Using CUDA[/green]")
    else:
        device = torch.device("cpu")
        console.print("[yellow]No GPU detected: Using CPU[/yellow]")
    return device

def get_optimal_batch_size(image_size, available_memory_gb=None):
    """
    Calculate optimal batch size based on available memory and image size.

    Args:
        image_size (tuple): Image dimensions (height, width)
        available_memory_gb (float): Available GPU memory in GB. If None, estimates based on system.

    Returns:
        int: Optimal batch size
    """
    # Estimate memory if not provided
    if available_memory_gb is None:
        if torch.backends.mps.is_available() or torch.cuda.is_available():
            available_memory_gb = 16  # Conservative estimate for GPU memory
        else:
            available_memory_gb = 8   # Conservative estimate for CPU memory

    # Calculate memory requirements per sample
    bytes_per_pixel = 4  # float32
    sample_memory = image_size[0] * image_size[1] * bytes_per_pixel

    # Reserve 20% of memory for model and other operations
    usable_memory = available_memory_gb * 1e9 * 0.2

    # Calculate batch size, with a hard cap of 128
    optimal_batch_size = min(128, int(usable_memory / sample_memory))

    # Ensure batch size is at least 16
    return max(16, optimal_batch_size)

def resume_training(trainer, checkpoint_path):
    """
    Resume training from a checkpoint if available.

    Args:
        trainer (ModelTrainer): Training instance
        checkpoint_path (Path): Path to checkpoint file
    """
    if checkpoint_path.exists():
        trainer.load_checkpoint(checkpoint_path)
        console.print(f"[green]Resumed training from {checkpoint_path}[/green]")
        return True
    return False

def train():
    """Main function for training the model using simple console output."""
    model_config = config.get("model", {})
    training_config = config.get("training", {})
    data_config = config.get("data", {})

    # Create output directory with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("outputs/letter_classifier") / timestamp
    os.makedirs(output_dir, exist_ok=True)

    # Save current config for reproducibility
    with open(output_dir / "config.yaml", "w") as f:
        yaml.dump(config, f, default_flow_style=False)

    # Calculate optimal batch size based on image dimensions
    optimal_batch_size = get_optimal_batch_size(model_config["input_shape"][:2])
    if optimal_batch_size != training_config["batch_size"]:
        console.print(
            f"[yellow]Adjusting batch size from {training_config['batch_size']} to {optimal_batch_size} based on available memory[/yellow]"
        )
        training_config["batch_size"] = optimal_batch_size

    # Create data loaders
    console.print("\n[bold cyan]Loading datasets...[/bold cyan]")
    # Assuming get_data_loaders is defined elsewhere
    data_loaders = get_data_loaders(
        data_dir="/content/drive/MyDrive/MScProject/data/characters",
        batch_size=training_config["batch_size"],
        image_size=model_config["input_shape"][:2],
        num_workers=4,
        augment=data_config.get("augmentation", {}).get("use", True)
    )

    # Print dataset statistics
    train_size = len(data_loaders["train"].dataset)
    val_size = len(data_loaders["val"].dataset)
    test_size = len(data_loaders["test"].dataset)
    console.print(f"\n[green]Dataset Statistics:[/green]")
    console.print(f"- Training set: {train_size:,} images")
    console.print(f"- Validation set: {val_size:,} images")
    console.print(f"- Test set: {test_size:,} images")
    console.print(f"- Batch size: {training_config['batch_size']}")

    # Create model (assuming get_model is defined and imported)
    console.print("\n[bold cyan]Initializing model...[/bold cyan]")
    model = get_model(model_config["architecture"], model_config)
    console.print(f"- Input shape: {model_config['input_shape']}")
    console.print(f"- Number of classes: {model_config['num_classes']}")
    console.print(f"- Model architecture: {model_config['architecture']}")

    # Set up device
    device = setup_device()

    # Create trainer (assuming ModelTrainer is defined and imported)
    trainer = ModelTrainer(
        model=model,
        train_loader=data_loaders["train"],
        test_loader=data_loaders["val"],
        config=config,
        output_dir=output_dir,
        device=device
    )

    # Optionally resume from checkpoint
    checkpoint_path = output_dir / "latest_checkpoint.pth"
    if resume_training(trainer, checkpoint_path):
        console.print("[green]Successfully resumed training from checkpoint[/green]")

    console.print("\n[bold cyan]Starting training...[/bold cyan]")
    # Training loop with simple console prints
    for epoch in range(1, training_config["epochs"] + 1):
        # Train
        train_loss, train_acc = trainer.train_epoch()
        # Validate
        val_loss, val_acc = trainer.validate()

        # Print epoch metrics
        console.print(
            f"Epoch {epoch}/{training_config['epochs']}: "
            f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | "
            f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | "
            f"LR: {trainer.optimizer.param_groups[0]['lr']:.6f}"
        )

        # Save checkpoint every 5 epochs
        if epoch % training_config.get("save_frequency", 5) == 0:
            trainer.save_checkpoint(epoch)
            console.print(f"[green]Checkpoint saved at epoch {epoch}[/green]")

        # Plot confusion matrix every 10 epochs (if implemented)
        # if epoch % 10 == 0:
        #     trainer.plot_confusion_matrix(predictions, targets, epoch)

    # Save final model and plots
    trainer.save_model("final")
    trainer.plot_history()

    console.print("\n[bold green]Training completed![/bold green]")
    console.print(f"Results saved to: {output_dir}")





# Train model
train()





# Create directory for results if it doesn't exist
!mkdir -p /content/drive/MyDrive/MScProject/results

# Copy results to Google Drive
!cp -r results/* /content/drive/MyDrive/MScProject/results
