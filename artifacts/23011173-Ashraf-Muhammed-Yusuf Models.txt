#---------------Notebook 1. Model for Character De-Obfuscation----------------
#-----------------------------------------------------------------------------
# Data generation pipeline
"""
Script for generating obfuscated (blurred, rotated, etc.) character images from regular character images
in the data/characters/regular directory.
This script reads the regular character images and creates corresponding obfuscated (blurred, rotated, etc.)
versions using the character mapping defined in the project.
"""

import os
import sys
import random
import argparse
import logging
import yaml
from pathlib import Path
from tqdm import tqdm
import numpy as np
import cv2

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate obfuscated character images from regular ones")

    parser.add_argument("--regular_dir", type=str, default="data/characters/regular",
                        help="Path to directory containing regular character images")
    parser.add_argument("--output_dir", type=str, default="data/characters/obfuscated",
                        help="Path to output directory for obfuscated images")
    parser.add_argument("--char_mapping_path", type=str, default="configs/character_mapping.yaml",
                        help="Path to character mapping YAML file")
    parser.add_argument("--num_samples_per_char", type=int, default=None,
                        help="Number of obfuscated samples to generate per character (default: same as regular)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed for reproducibility")

    return parser.parse_args()


def load_character_mapping(mapping_path):
    """Load character mapping from YAML file."""
    if os.path.exists(mapping_path):
        with open(mapping_path, 'r', encoding='utf-8') as f:
            mapping = yaml.safe_load(f)

        # Filter out any comments or special sections
        if mapping:
            mapping = {k: v for k, v in mapping.items() if isinstance(
                k, str) and isinstance(v, list)}
        return mapping
    else:
        logger.warning(f"Character mapping file not found at {mapping_path}")
        return None


def apply_random_transformation(image):
    """
    Apply random transformations to an image to make it look different
    while preserving its general structure.

    Args:
        image: Input image

    Returns:
        Transformed image
    """
    # Make a copy to avoid modifying the original
    transformed = image.copy()

    # Random rotation (small angle)
    if random.random() > 0.5:
        angle = random.uniform(-10, 10)
        h, w = transformed.shape[:2]
        center = (w // 2, h // 2)
        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
        transformed = cv2.warpAffine(transformed, rotation_matrix, (w, h),
                                     borderMode=cv2.BORDER_CONSTANT, borderValue=255)

    # Random noise
    if random.random() > 0.7:
        noise = np.random.normal(0, 10, transformed.shape).astype(np.uint8)
        transformed = cv2.add(transformed, noise)

    # Random blur
    if random.random() > 0.7:
        kernel_size = random.choice([3, 5])
        transformed = cv2.GaussianBlur(
            transformed, (kernel_size, kernel_size), 0)

    # Random dilation/erosion
    if random.random() > 0.7:
        kernel_size = random.randint(1, 2)
        kernel = np.ones((kernel_size, kernel_size), np.uint8)
        if random.random() > 0.5:
            transformed = cv2.dilate(transformed, kernel, iterations=1)
        else:
            transformed = cv2.erode(transformed, kernel, iterations=1)

    # Random brightness adjustment
    if random.random() > 0.5:
        alpha = random.uniform(0.8, 1.2)  # Brightness factor
        transformed = cv2.convertScaleAbs(transformed, alpha=alpha, beta=0)

    return transformed


def main():
    """Main function for generating obfuscated character images."""
    args = parse_args()
    random.seed(args.seed)
    np.random.seed(args.seed)

    # Load character mapping
    char_mapping = load_character_mapping(args.char_mapping_path)
    if not char_mapping:
        logger.error("Could not load character mapping. Exiting.")
        return

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    # Process each split (train, test)
    for split in ['train', 'test']:
        regular_split_dir = os.path.join(args.regular_dir, split)
        output_split_dir = os.path.join(args.output_dir, split)

        if not os.path.exists(regular_split_dir):
            logger.warning(f"Directory not found: {regular_split_dir}")
            continue

        os.makedirs(output_split_dir, exist_ok=True)

        # Process each character directory
        for char_dir in os.listdir(regular_split_dir):
            char_path = os.path.join(regular_split_dir, char_dir)
            if not os.path.isdir(char_path):
                continue

            # Create corresponding output directory
            output_char_dir = os.path.join(output_split_dir, char_dir)
            os.makedirs(output_char_dir, exist_ok=True)

            # Get list of image files
            image_files = [f for f in os.listdir(
                char_path) if f.endswith('.png')]

            # Limit the number of samples if specified
            if args.num_samples_per_char and len(image_files) > args.num_samples_per_char:
                image_files = random.sample(
                    image_files, args.num_samples_per_char)

            logger.info(
                f"Processing {len(image_files)} images for character '{char_dir}' in {split} split")

            # Process each image
            for img_file in tqdm(image_files, desc=f"Char '{char_dir}'"):
                img_path = os.path.join(char_path, img_file)

                # Load the image
                try:
                    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                    if image is None:
                        logger.warning(f"Could not load image: {img_path}")
                        continue
                except Exception as e:
                    logger.error(f"Error loading image {img_path}: {e}")
                    continue

                # Apply random transformations to create an "obfuscated" version
                obfuscated_img = apply_random_transformation(image)

                # Save the obfuscated image with the same filename
                output_path = os.path.join(output_char_dir, img_file)
                cv2.imwrite(output_path, obfuscated_img)

            logger.info(
                f"Generated {len(image_files)} obfuscated images for character '{char_dir}'")

    logger.info(
        f"Obfuscated image generation completed, saved to {args.output_dir}")


if __name__ == "__main__":
    main()


"""
Script for generating obfuscated character images using fonts from fonts/fonts_obfuscated.txt.
This script creates one image per obfuscated variant per font in the fonts/fonts_obfuscated.txt file.
"""

import os
import sys
import random
import argparse
import logging
import yaml
from pathlib import Path
import platform
from tqdm import tqdm
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFont

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate obfuscated character variants with fonts from fonts/fonts_obfuscated.txt")

    parser.add_argument("--output_dir", type=str, default="data/characters/obfuscated/train",
                        help="Path to output directory for obfuscated images")
    parser.add_argument("--char_mapping_path", type=str, default="configs/character_mapping.yaml",
                        help="Path to character mapping YAML file")
    parser.add_argument("--font_list_path", type=str, default="fonts/fonts_obfuscated.txt",
                        help="Path to the font list file")
    parser.add_argument("--image_size", type=int, default=64,
                        help="Size of the output images (square)")
    parser.add_argument("--font_size", type=int, default=36,
                        help="Font size to use for rendering characters")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed for reproducibility")

    return parser.parse_args()


def load_character_mapping(mapping_path):
    """Load character mapping from YAML file."""
    if os.path.exists(mapping_path):
        with open(mapping_path, 'r', encoding='utf-8') as f:
            mapping = yaml.safe_load(f)

        # Filter out any comments or special sections
        if mapping:
            mapping = {k: v for k, v in mapping.items() if isinstance(
                k, str) and isinstance(v, list)}
        return mapping
    else:
        logger.warning(f"Character mapping file not found at {mapping_path}")
        return None


def load_font_list(font_list_path):
    """
    Load the list of fonts from the font list file.
    Skips comments and empty lines.

    Args:
        font_list_path: Path to the font list file

    Returns:
        List of font names
    """
    fonts = []
    if not os.path.exists(font_list_path):
        logger.error(f"Font list file not found: {font_list_path}")
        return fonts

    with open(font_list_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            # Skip empty lines and comments
            if not line or line.startswith('#'):
                continue
            fonts.append(line)

    logger.info(f"Loaded {len(fonts)} fonts from {font_list_path}")
    return fonts


def find_system_fonts():
    """Find system fonts based on the operating system."""
    system = platform.system()
    font_paths = []

    if system == 'Darwin':  # macOS
        font_dirs = [
            "/System/Library/Fonts",
            "/Library/Fonts",
            os.path.expanduser("~/Library/Fonts")
        ]
    elif system == 'Windows':
        font_dirs = [
            "C:\\Windows\\Fonts"
        ]
    else:  # Linux and others
        font_dirs = [
            "/usr/share/fonts",
            "/usr/local/share/fonts",
            os.path.expanduser("~/.fonts")
        ]

    # Common font extensions
    extensions = ['.ttf', '.otf', '.ttc', '.dfont']

    # Find all font files in the directories
    for font_dir in font_dirs:
        if os.path.exists(font_dir):
            for root, _, files in os.walk(font_dir):
                for file in files:
                    if any(file.lower().endswith(ext) for ext in extensions):
                        font_paths.append(os.path.join(root, file))

    return font_paths


def map_font_names_to_files(requested_fonts):
    """
    Map font names from the font list to actual font files.

    Args:
        requested_fonts: List of font names to match

    Returns:
        Dictionary mapping font names to font file paths
    """
    system_fonts = find_system_fonts()
    logger.info(f"Found {len(system_fonts)} font files on the system")

    font_map = {}
    for font_name in requested_fonts:
        matched = False
        # Look for an exact match first
        for font_path in system_fonts:
            filename = os.path.basename(font_path).lower()
            font_name_lower = font_name.lower()
            if font_name_lower in filename:
                font_map[font_name] = font_path
                matched = True
                break

        if not matched:
            # Look for partial matches if no exact match
            for font_path in system_fonts:
                # Split font name by spaces and look for parts
                parts = font_name.lower().split()
                filename = os.path.basename(font_path).lower()
                if any(part in filename for part in parts if len(part) > 2):
                    font_map[font_name] = font_path
                    matched = True
                    break

        if not matched:
            logger.warning(
                f"Could not find a matching font file for '{font_name}'")

    logger.info(
        f"Mapped {len(font_map)} out of {len(requested_fonts)} requested fonts")
    return font_map


def test_font_for_character(font_path, character):
    """Test if a font can properly render a character."""
    try:
        font = ImageFont.truetype(font_path, 20)
        img = Image.new('L', (30, 30), color=255)
        draw = ImageDraw.Draw(img)
        draw.text((10, 10), character, fill=0, font=font)
        # Check if the image actually contains something
        array = np.array(img)
        if np.sum(255 - array) < 10:  # Almost blank image
            return False
        return True
    except Exception:
        return False


def render_character(character, font_path, image_size=64, font_size=36):
    """
    Render a character as an image with specified font.

    Args:
        character: The character to render
        font_path: Path to the font file
        image_size: Size of the output image (square)
        font_size: Font size to use

    Returns:
        np.ndarray: Grayscale image of the rendered character
    """
    # Create a blank image with white background
    img = Image.new('L', (image_size, image_size), color=255)
    draw = ImageDraw.Draw(img)

    try:
        # Try to use the specified font
        try:
            font = ImageFont.truetype(font_path, font_size)
        except Exception as e:
            logger.debug(f"Error loading font {font_path}: {e}")
            font = ImageFont.load_default()

        # Calculate text position to center it
        try:
            text_width, text_height = font.getsize(character)
        except AttributeError:
            # For newer PIL versions
            text_width, text_height = font.getbbox(character)[2:4]

        x = (image_size - text_width) // 2
        y = (image_size - text_height) // 2

        # Draw the character
        draw.text((x, y), character, fill=0, font=font)

        # Convert to numpy array
        image_array = np.array(img)

        return image_array

    except Exception as e:
        logger.debug(
            f"Error rendering character '{character}' with font {font_path}: {e}")
        # Return a blank image in case of error
        return np.ones((image_size, image_size), dtype=np.uint8) * 255


def create_directory_structure(output_dir):
    """Create the directory structure for the dataset."""
    os.makedirs(output_dir, exist_ok=True)

    # Create directories for each letter (a-z)
    for char in 'abcdefghijklmnopqrstuvwxyz':
        char_dir = os.path.join(output_dir, char)
        os.makedirs(char_dir, exist_ok=True)


def generate_images(output_dir, char_mapping, font_map, image_size=64, font_size=36):
    """
    Generate obfuscated character images for each variant with each font.

    Args:
        output_dir: Output directory for the generated images
        char_mapping: Dictionary mapping standard characters to obfuscated variants
        font_map: Dictionary mapping font names to font file paths
        image_size: Size of output images
        font_size: Font size to use

    Returns:
        Total number of images generated
    """
    total_generated = 0
    total_expected = 0

    # For each lowercase letter
    for char in 'abcdefghijklmnopqrstuvwxyz':
        if char not in char_mapping:
            logger.warning(f"No mapping found for character '{char}'")
            continue

        variants = char_mapping[char]
        expected_count = len(variants) * len(font_map)
        total_expected += expected_count

        logger.info(
            f"Generating {len(variants)} variants with {len(font_map)} fonts each for character '{char}'")

        char_dir = os.path.join(output_dir, char)

        # For each variant
        for variant_idx, variant in enumerate(variants):
            # For each font
            for font_idx, (font_name, font_path) in enumerate(tqdm(font_map.items(), desc=f"Char '{char}' var {variant_idx+1}")):
                try:
                    # Check if this font can render this character
                    if not test_font_for_character(font_path, variant):
                        continue

                    # Render the variant character with this font
                    img = render_character(
                        variant,
                        font_path,
                        image_size=image_size,
                        font_size=font_size
                    )

                    # Determine file name for the variant
                    if any(c in variant for c in r'\/:|<>"?*'):
                        # For variants with special characters that might cause filename issues, use indices
                        output_path = os.path.join(
                            char_dir, f"variant_{variant_idx+1}_font_{font_idx+1}.png")
                    else:
                        safe_font_name = font_name.replace(' ', '_').replace(
                            ',', '').replace('(', '').replace(')', '')[:30]
                        output_path = os.path.join(
                            char_dir, f"variant_{variant_idx+1}_{variant}_{safe_font_name}.png")

                    # Save the image
                    cv2.imwrite(output_path, img)
                    total_generated += 1

                except Exception as e:
                    logger.error(
                        f"Error creating image for variant '{variant}' with font {font_name}: {e}")

        logger.info(f"Completed generation for character '{char}'")

    logger.info(
        f"Generated {total_generated} out of {total_expected} expected images")
    return total_generated


def main():
    """Main function for generating obfuscated character images."""
    args = parse_args()
    random.seed(args.seed)
    np.random.seed(args.seed)

    # Load character mapping
    char_mapping = load_character_mapping(args.char_mapping_path)
    if not char_mapping:
        logger.error("Could not load character mapping. Exiting.")
        return

    # Load font list
    font_list = load_font_list(args.font_list_path)
    if not font_list:
        logger.error("No fonts found in the font list. Exiting.")
        return

    # Map font names to font files
    font_map = map_font_names_to_files(font_list)
    if not font_map:
        logger.error("Could not map any font names to font files. Exiting.")
        return

    # Create directory structure
    create_directory_structure(args.output_dir)

    # Generate images
    total_generated = generate_images(
        args.output_dir,
        char_mapping,
        font_map,
        args.image_size,
        args.font_size
    )

    logger.info(
        f"Obfuscated image generation completed. Generated {total_generated} images.")
    logger.info(f"Images saved to {args.output_dir}")


if __name__ == "__main__":
    main()



# Install dependencies
!pip install -q tensorflow matplotlib
# Import dependencies
import os
import glob
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from google.colab import drive
from tensorflow.keras import layers, models, callbacks, mixed_precision
from sklearn.metrics import classification_report, confusion_matrix
# Mount Drive so you can read datasets and write checkpoints
# Link to dataset:
# https://drive.google.com/drive/folders/1sfNG1PkmTPBe1wOSQXZmfdkvR97Hn9lk?usp=sharing
drive.mount('/content/drive')
# (Optional—but useful) turn on XLA JIT for extra speed
tf.config.optimizer.set_jit(True)

BATCH = 64
IMG_SIZE = (64, 64)
BASE_DIR="/content/drive/MyDrive/MScProject"
CKPT_DIR=f"{BASE_DIR}/char_ckpt_best.keras"
train_dir = f"{BASE_DIR}/data/characters/train"
test_dir = f"{BASE_DIR}/data/characters/test"

# Train dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  train_dir,
  labels="inferred",
  label_mode="categorical",
  batch_size=BATCH,
  image_size=IMG_SIZE,
  color_mode="grayscale",
  validation_split=0.20,
  subset="training",
  seed=42
)
# Val dataset
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  train_dir,
  labels="inferred",
  label_mode="categorical",
  batch_size=BATCH,
  image_size=IMG_SIZE,
  color_mode="grayscale",
  validation_split=0.20,
  subset="validation",
  seed=42
)
# Test dataset
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
  test_dir,
  labels="inferred",
  label_mode="categorical",
  batch_size=BATCH,
  image_size=IMG_SIZE,
  color_mode="grayscale",
  shuffle=False
)

# Utility to display examples from each set
def show_examples(ds, ds_name, num=5):
  # Take one batch
  for images, labels in ds.take(1):
    images = images.numpy()
    labels = labels.numpy()
    class_names = ds.class_names
    break
  plt.figure(figsize=(6,6))
  for i in range(num):
    ax = plt.subplot(3, 3, i+1)
    img = images[i].squeeze()  # shape: (H,W) since grayscale
    lbl = class_names[labels[i].argmax()]
    plt.imshow(img, cmap='gray')
    plt.title(f"{ds_name}: {lbl}")
    plt.axis('off')
  plt.tight_layout()
  plt.show()
# Display 5 examples from each split
show_examples(train_ds, "Train")
show_examples(val_ds, "Val")
show_examples(test_ds, "Test")

# Save dataset class names before piping the dataset through 'map'
train_ds_class_names = test_ds.class_names
val_ds_class_names = test_ds.class_names
test_ds_class_names = test_ds.class_names
# Normalize and augment datasets (only the train dataset is augmented)
normalization = layers.Rescaling(1./255)
aug = tf.keras.Sequential([
  layers.RandomRotation(0.1),
  layers.RandomZoom(0.1),
  layers.RandomTranslation(0.1, 0.1)
])
train_ds = train_ds.map(lambda x,y: (aug(normalization(x)), y))
val_ds   = val_ds.map(lambda x,y: (normalization(x), y))
test_ds  = test_ds.map(lambda x,y: (normalization(x), y))

# Define model
# 1. Input Layer
inputs = layers.Input(shape=(*IMG_SIZE, 1))
# 2. (Conv + ReLU) + Pooling 1
x = layers.Conv2D(32, 3, activation='relu')(inputs)
x = layers.MaxPooling2D()(x)
# 3. (Conv + ReLU) + Pooling 2
x = layers.Conv2D(64, 3, activation='relu')(x)
x = layers.MaxPooling2D()(x)
# 4. (Conv + ReLU)
x = layers.Conv2D(128, 3, activation='relu')(x)
# 5. Flatten to Vector
x = layers.Flatten()(x)
# 6. (FC + ReLU) Layer
x = layers.Dense(128, activation='relu')(x)
# 7. Dropout Regularisation
x = layers.Dropout(0.5)(x)
# 8. Output Layer
outputs = layers.Dense(26, activation='softmax')(x)
# Construct model
model = models.Model(inputs, outputs)
# Show model summary
model.summary()

# Compile model
model.compile(
  optimizer=tf.keras.optimizers.Adam(1e-3),
  loss='categorical_crossentropy',
  metrics=['accuracy']
)

# Callbacks
# 1. Checkpoint - saves the best model
ckpt = callbacks.ModelCheckpoint(
  filepath=CKPT_DIR,
  save_best_only=True,
  monitor="val_loss" # keep only the best model
)
# 2. Early stopping
es = callbacks.EarlyStopping(
  monitor="val_loss",
  patience=6, # stop ~6 epochs after val_loss stalls
  restore_best_weights=True
)
# 3. LR scheduler
lr_s = callbacks.ReduceLROnPlateau(
  monitor="val_loss",
  factor=0.5,
  patience=3, # halve LR if val_loss hasn’t improved for 3 epochs
  min_lr=1e-6
)
callbacks=[ckpt, es, lr_s]

# Train model
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=50,
  callbacks=callbacks
)

# Load best checkpoint's weights
model.load_weights(CKPT_DIR)
# Test model accuracy on test dataset
model.evaluate(test_ds)
# Training curves
epochs = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(12, 4))
# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs, history.history['accuracy'],    label='train_acc')
plt.plot(epochs, history.history['val_accuracy'],label='val_acc')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.legend()
# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs, history.history['loss'],    label='train_loss')
plt.plot(epochs, history.history['val_loss'],label='val_loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.legend()

# Gather all ground-truths and predictions
y_true = []
y_pred = []
for batch_x, batch_y in test_ds:
  preds = model.predict(batch_x)
  y_pred.extend(np.argmax(preds, axis=1))
  y_true.extend(np.argmax(batch_y.numpy(), axis=1))
class_names = test_ds_class_names
print(classification_report(y_true, y_pred, target_names=class_names))
cm = confusion_matrix(y_true, y_pred)
# Plot confusion matrix
plt.figure(figsize=(10, 10))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.colorbar()
tick_marks = np.arange(len(train_ds_class_names))
plt.xticks(tick_marks, train_ds_class_names, rotation=90)
plt.yticks(tick_marks, train_ds_class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()

# Display misclassified examples
images_all = np.concatenate([x.numpy() for x, y in test_ds], axis=0)
mis_idx = [i for i, (t, p) in enumerate(zip(y_true, y_pred)) if t!=p]
plt.figure(figsize=(9, 9))
for i, idx in enumerate(mis_idx[:9]):
  plt.subplot(3, 3, i+1)
  img = images_all[idx].squeeze()
  plt.imshow(img, cmap='gray')
  plt.title(f"T:{class_names[y_true[idx]]} P:{class_names[y_pred[idx]]}")
  plt.axis('off')
plt.tight_layout()
plt.show()





#----Notebook 2. CNN Sliding-Window Model for 3-Letter Word De-Obfuscation----
#-----------------------------------------------------------------------------
# Install dependencies
!pip install -q tensorflow matplotlib
# Optional - Install font (we will use it to generate images)
# Colab / Ubuntu repositories already ship Roboto
!sudo apt-get -qq update
!sudo apt-get -qq install fonts-roboto
# Import dependencies
import os, sys, random, itertools, pathlib, math, shutil, io, requests
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tqdm as tq
from tqdm import tqdm
from pathlib import Path
from glob import glob
from tensorflow.keras import layers, models, mixed_precision, backend as K
from google.colab import drive
from sklearn.metrics import classification_report, confusion_matrix
from collections import defaultdict
from PIL import Image, ImageDraw, ImageFont
from typing import Tuple
# 1.3 Mount Drive & define base path
# Mount Drive so you can read datasets and write checkpoints
# Link to Drive:
# https://drive.google.com/drive/folders/1sfNG1PkmTPBe1wOSQXZmfdkvR97Hn9lk?usp=sharing
drive.mount('/content/drive')

# paths & constants
BASE_PATH         = Path("/content/drive/MyDrive/MScProject")
GLYPH_DIR         = Path(f"{BASE_PATH}/data/characters/train")
DATA_ROOT         = Path(f"{BASE_PATH}/data/words3")
CKPT_DIR          = f"{BASE_PATH}/words3_ckpt_best.keras"
BATCH             = 64
IMG_H             = IMG_W = 64
IMG_SHAPE         = (IMG_H, IMG_W)
PATCH_W           = IMG_W // 3
VARIANTS_PER      = 5 # per word
EXPECTED_CLASSES  = 26**3 # 26³ = 17,576
FINAL_TEST_FRAC   = 0.20
SEED              = 42
PATCH_W           = IMG_W // 3 # 21 when IMG_W = 64
N_VARIANTS        = 4 # number of images per class
FRACTION          = 0.15 # 15 %
train_dir         = DATA_ROOT / "train"
test_dir          = DATA_ROOT / "test"
random.seed(SEED)

"""
- Generates a single ‘train/’ directory with 17,576 class folders (AAA … ZZZ)
- Each class contains N_VARIANTS PNG images rendered on-the-fly (no external glyph reuse)
- Obfuscation applied per-character (leet + random spacing jitter)
- Idempotent: if the train folder already has 17,576 classes it exits immediately
"""
def obfuscate_char(ch: str) -> str:
    mode = random.choices(("plain", "leet"), weights=(0.5, 0.4, 0.1))[0]
    if mode == "leet" and ch in LEET:
        return random.choice(LEET[ch])
    return ch
def render_patch(ch: str) -> Image.Image:
    # Return a 64×21 monochrome patch for a single (possibly obfuscated) char
    patch = Image.new("L", (PATCH_W, IMG_H), color=255)
    draw  = ImageDraw.Draw(patch)
    draw.text((4, 4), obfuscate_char(ch), fill=0, font=FONT)
    return patch
def stitch_word(word: str, out_file: Path):
  canvas = Image.new("L", (IMG_W, IMG_H), color=255)
  for idx, ch in enumerate(word):
      glyph = render_patch(ch)
      canvas.paste(glyph, (idx * PATCH_W, 0))
  # light horizontal jitter
  if random.random() < 0.3:
      dx = random.randint(-2, 2)
      canvas = canvas.transform(canvas.size, Image.AFFINE, (1, 0, dx, 0, 1, 0))
  canvas.save(out_file)
AAA = train_dir / "AAA"
dataset_ready = AAA.is_dir() and any(AAA.glob("*.png"))
if dataset_ready:
    print("words3/train already complete – nothing to do.")
else:
    # Define font
    try:
        FONT_PATH = "/usr/share/fonts/truetype/roboto/Roboto-Medium.ttf"
        # A big font size makes the letters cut-off at the edges
        # when we slice through the images,
        # mimicking real-world scenarios where this operation
        # may not produce clean cuts of letters
        FONT_SIZE = 40
        FONT = ImageFont.truetype(FONT_PATH, FONT_SIZE)
        print("Using Roboto Medium font to generate images")
    except Exception as e:
        print(f"Error loading font: {e}")
        print("Using default font to generate images")
        FONT = ImageFont.load_default()
    # mapping tables (uppercase only)
    LEET = {
      'A': ['Α', '4', 'Д', 'Ä', 'Á', 'À', 'Â', '@', 'Δ'],
      'B': ['8', 'β', 'Β', 'В'],
      'C': ['Ç', 'Ć', 'Č', 'С'],
      'D': ['Ð', 'Ď'],
      'E': ['3', 'Σ', 'Έ', 'Ε', 'Е', 'Ë', 'É', 'È', 'Ê'],
      'F': ['Φ', 'Ƒ'],
      'G': ['6', 'Ğ', 'Ģ', 'Γ'],
      'H': ['Η', 'Н'],
      'I': ['1', '|', 'Í', 'Ì', 'Î', 'Ï', 'И'],
      'J': ['Ј'],
      'K': ['Κ', 'К'],
      'L': ['Ι', 'Ł', 'Ĺ', 'Л'],
      'M': ['Μ', 'М'],
      'N': ['Ν', 'Ń', 'Ñ', 'Н'],
      'O': ['0', 'Θ', 'Ο', 'Ө', 'Ø', 'Ö', 'Ó', 'Ò', 'Ô'],
      'P': ['Ρ', 'Р'],
      'Q': ['Φ'],
      'R': ['®', 'Я', 'Ř', 'Ŕ'],
      'S': ['5', '$', 'Ѕ', 'Ś', 'Š'],
      'T': ['Τ', 'Т'],
      'U': ['Υ', 'Ц', 'Ü', 'Ú', 'Ù', 'Û'],
      'V': ['Ѵ', 'V'],
      'W': ['Ω', 'Ѡ', 'Ψ', 'Ш', 'Щ'],
      'X': ['Χ', 'Ж', 'Х'],
      'Y': ['Υ', 'Ү', 'Ý', 'Ÿ'],
      'Z': ['Ζ', 'Ż', 'Ź', 'Ž', 'З', '2']
    }
    # wipe & rebuild train directory (safe for colab runs)
    if train_dir.exists():
        shutil.rmtree(train_dir)
    train_dir.mkdir(parents=True, exist_ok=True)
    # generate every word (AAA … ZZZ)
    alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    all_words = ["".join(tpl) for tpl in itertools.product(alphabet, repeat=3)]
    for word in tqdm(all_words, desc="Generating train"):
        cls_dir = train_dir / word
        cls_dir.mkdir(parents=True, exist_ok=True)
        for k in range(N_VARIANTS):
            stitch_word(word, cls_dir / f"{word}_{k}.png")
    print("Training set complete.")

"""
- Make a permanent 15% test split on Drive
- Assumes you have a single words3/train/AAA … ZZZ/*.png structure already.
- Creates /words3/test/AAA … ZZZ/ and MOVES files (no duplication).
- Safe to rerun – will skip classes already processed.
"""
import tqdm
AAA = test_dir / "AAA"
dataset_ready = AAA.is_dir() and any(AAA.glob("*.png"))
if dataset_ready:
    print("words3/test already complete – nothing to do.")
else:
    test_dir.mkdir(parents=True, exist_ok=True)

    # split loop
    for cls_dir in tqdm.tqdm([d for d in train_dir.iterdir() if d.is_dir()], desc="Creating 15 % test split"):
        tgt_cls = test_dir / cls_dir.name
        tgt_cls.mkdir(parents=True, exist_ok=True)

        # list PNGs still in train/ for this class (those already moved last run are gone)
        imgs = list(cls_dir.glob("*.png"))
        if not imgs: # all imgs already moved in a previous run
            continue

        # number to move: 15% rounded down, but keep ≥1 in train/
        n_move = max(1, math.floor(len(imgs) * FRACTION))
        n_move = min(n_move, len(imgs) - 1) # safeguard: leave ≥1

        random.shuffle(imgs)
        for img in imgs[:n_move]:
            shutil.move(str(img), tgt_cls / img.name)
    print("Test split ready.")
    print("Train images:", sum(1 for _ in train_dir.rglob("*.png")))
    print("Test images:", sum(1 for _ in test_dir.rglob("*.png")))

def normalise(img, label):
    img = tf.cast(img, tf.float32) / 255.0
    return img, label

# Train dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    labels="inferred",
    label_mode="categorical",
    batch_size=BATCH,
    image_size=IMG_SHAPE,
    color_mode="grayscale",
    validation_split=0.2,
    subset="training",
    shuffle=True,
    seed=42
)
# Val dataset
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    labels="inferred",
    label_mode="categorical",
    batch_size=BATCH,
    image_size=IMG_SHAPE,
    color_mode="grayscale",
    validation_split=0.2,
    subset="validation",
    shuffle=False,
    seed=42
)
# Test dataset
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    labels="inferred",
    label_mode="categorical",
    batch_size=BATCH,
    image_size=IMG_SHAPE,
    color_mode="grayscale",
    shuffle=False
)

# Generate class names for future reference
class_names = train_ds.class_names
# Preprocess datasets
train_ds = (train_ds
            .map(normalise, num_parallel_calls=tf.data.AUTOTUNE)
            .apply(tf.data.experimental.ignore_errors())
            .prefetch(tf.data.AUTOTUNE))
val_ds = (val_ds
          .map(normalise, num_parallel_calls=tf.data.AUTOTUNE)
          .apply(tf.data.experimental.ignore_errors())
          .prefetch(tf.data.AUTOTUNE))
test_ds = (test_ds
           .map(normalise, num_parallel_calls=tf.data.AUTOTUNE)
           .apply(tf.data.experimental.ignore_errors())
           .prefetch(tf.data.AUTOTUNE))

# Utility to display examples from each set
def show_examples(ds, ds_name, num=5):
  # Take one batch
  for images, labels in ds.take(1):
      images = images.numpy()
      labels = labels.numpy()
      break

  plt.figure(figsize=(6,6))

  for i in range(num):
      ax = plt.subplot(3, 3, i+1)
      img = images[i].squeeze()  # shape: (H,W) since grayscale
      lbl = class_names[labels[i].argmax()]
      plt.imshow(img, cmap='gray')
      plt.title(f"{ds_name}: {lbl}")
      plt.axis('off')

  plt.tight_layout()
  plt.show()

# Display 5 examples from each split
show_examples(train_ds, "Train")
show_examples(val_ds, "Val")
show_examples(test_ds, "Test")

base_model = models.load_model(f"{BASE_PATH}/char_cnn_ckpt_best.keras")
base_model.trainable = False # freeze weights initially
print("Base model frozen — params:", base_model.count_params())

class UnfreezeAndFineTune(tf.keras.callbacks.Callback):
    def __init__(self, base_model, n_blocks=1,
                 new_lr=1e-4, patience=4):
        super().__init__()
        self.base_model = base_model
        self.n_blocks = n_blocks
        self.new_lr = new_lr
        self.patience = patience
        self.wait = 0
        self.best = None
        self.unfroze = False

    def unfreeze_last_conv_blocks(self, N=1):
        # 1) Freeze everything first
        for layer in self.base_model.layers:
            layer.trainable = False

        # 2) Collect indices of all Conv2D layers
        conv_idx = [idx for idx, layer in enumerate(self.base_model.layers)
                    if isinstance(layer, tf.keras.layers.Conv2D)]

        # 3) Decide which indices to unfreeze (last N)
        if N > len(conv_idx):
            raise ValueError(f"Model only has {len(conv_idx)} Conv2D layers, "
                            f"cannot unfreeze {N}")

        to_unfreeze = conv_idx[-N:]

        # 4) Unfreeze selected Conv2D layers *and* everything that follows them
        #    (so the gradient flows through BN / ReLU / Dense that depend on them)
        for idx in to_unfreeze:
            for layer in self.base_model.layers[idx:]:
                layer.trainable = True

        print(f"Unfroze {N} Conv2D block(s) starting with layer(s):",
              [self.base_model.layers[i].name for i in to_unfreeze])

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get("val_loss")
        if current is None:
            return

        # first time: set best
        if self.best is None:
            self.best = current
            return

        if current < self.best:
            self.best = current
            self.wait = 0
        else:
            self.wait += 1

        # when patience exceeded, unfreeze
        if (self.wait >= self.patience) and not self.unfroze:
            print(f"\nPatience of {self.patience} reached. Unfreezing top {self.n_blocks} block(s).")

            # Unfreeze last N conv blocks
            self.unfreeze_last_conv_blocks(N=1)

            # lower LR and recompile
            self.model.compile(
                optimizer=tf.keras.optimizers.Adam(self.new_lr),
                loss="categorical_crossentropy",
                metrics=["accuracy"],
            )
            self.unfroze = True
            print(f"Recompiled with lr={self.new_lr}. Now continuing training.")

def extract_patch(x, idx):
    start = idx * PATCH_W
    return x[:, :, start:start+PATCH_W, :] # (None, 64, 21, 1)

inputs = tf.keras.Input(shape=(IMG_H, IMG_W, 1))
logits = []

for i in range(3):
    patch = layers.Lambda(lambda z, i=i: extract_patch(z, i))(inputs)
    patch = layers.Resizing(IMG_H, IMG_H)(patch) # -> (64 x 64 x 1)
    # Re-use frozen base_model (shared weights)
    logits.append(base_model(patch)) # (None, 26)

concat = layers.Concatenate()(logits) # (None, 78)

# FC (+ ReLU) layers & dropout regularisation
x = layers.Dense(512, activation="relu")(concat)
x = layers.Dropout(0.5)(x)
x = layers.Dense(256, activation="relu")(x)
x = layers.Dropout(0.3)(x)

# Output
outputs = layers.Dense(EXPECTED_CLASSES, activation='softmax')(x)

# Create model & print summary
model = models.Model(inputs, outputs)
model.summary()

# Compile model
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callbacks
# 1. Checkpoint
ckpt = tf.keras.callbacks.ModelCheckpoint(
    CKPT_DIR,
    save_best_only=True, # keep only the best model
    monitor='val_loss'
)

# 2. Early stopping
es = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5, # stop ~5 epochs after val_loss stalls
    restore_best_weights=True
)

# 3. Unfreeze last N conv blocks if
unfreeze = UnfreezeAndFineTune(
    base_model=base_model,
    n_blocks=1,      # how many conv-blocks to unfreeze
    new_lr=1e-4,     # lower LR for fine-tuning
    patience=4       # same as EarlyStopping patience
)

# 4. LR Scheduler
lr_s = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=2,
    min_lr=1e-6
)

cb = [ckpt, es, unfreeze]

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=30,
    callbacks=cb
)

# (Optional) manually unfreeze deepest N conv layers and conbtinue training
unf = UnfreezeAndFineTune(
    base_model=base_model,
    n_blocks=1,      # how many conv-blocks to unfreeze
    new_lr=1e-4,     # lower LR for fine-tuning
    patience=4       # same as EarlyStopping patience
)

unf.unfreeze_last_conv_blocks(N=1)
model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

initial_epochs = history.epoch[-1]
history_unf = model.fit(
    train_ds,
    validation_data=val_ds,
    initial_epoch=initial_epochs,
    epochs=initial_epochs + 20,
    callbacks=[ckpt, es, lr_s]
)

def merge_histories(h1: tf.keras.callbacks.History, h2: tf.keras.callbacks.History) -> tf.keras.callbacks.History:
    merged = tf.keras.callbacks.History()
    merged.history = {}
    # assume both histories tracked the same keys
    for k in h1.history.keys():
        vals1 = h1.history[k]
        vals2 = h2.history.get(k, [])
        merged.history[k] = vals1 + vals2
    return merged

# Merge both training histories
combined_history = merge_histories(history, history_unf)

# Load best checkpoint's weights
model.load_weights(CKPT_DIR)

test_loss, test_acc = model.evaluate(test_ds)
print(f"Test accuracy: {test_acc:.4f}")

# Training curves
epochs = range(1, len(combined_history.history['loss']) + 1)
plt.figure(figsize=(12, 4))

# Accuracy - frozen base model
plt.subplot(1, 2, 1)
plt.plot(epochs, combined_history.history['accuracy'],    label='train_acc')
plt.plot(epochs, combined_history.history['val_accuracy'],label='val_acc')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs, combined_history.history['loss'],    label='train_loss')
plt.plot(epochs, combined_history.history['val_loss'],label='val_loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.legend()

# Gather all ground-truths and predictions
y_true = []
y_pred = []
for batch_x, batch_y in test_ds:
  preds = model.predict(batch_x)
  y_pred.extend(np.argmax(preds, axis=1))
  y_true.extend(np.argmax(batch_y.numpy(), axis=1))

cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 10))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.colorbar()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names, rotation=90)
plt.yticks(tick_marks, class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()





#--------------------------Notebook 3. Tesseract OCR---------------------------
#------------------------------------------------------------------------------
# Install / import dependencies
!apt-get update && apt-get install -y tesseract-ocr
!pip install -q pytesseract pillow jiwer
import os
import glob
import pytesseract
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
from google.colab import drive
from PIL import Image
from jiwer import wer, cer
from tqdm import tqdm
from collections import Counter
# Mount Drive & define base path
# Mount Drive so you can read datasets and write checkpoints
# Link to Drive:
# https://drive.google.com/drive/folders/1sfNG1PkmTPBe1wOSQXZmfdkvR97Hn9lk?usp=sharing
drive.mount('/content/drive')

# Tesseract over Character Dataset
CHAR_TEST_DIR = "/content/drive/MyDrive/MScProject/data/characters/test"
# Tell pytesseract to treat each image as a single character, restrict to A–Z
TESSERACT_CONFIG = r"--psm 10 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ"
# Run through the dataset
y_true, y_pred = [], []
for true_char in sorted(os.listdir(CHAR_TEST_DIR)):
    char_dir = Path(CHAR_TEST_DIR) / true_char
    if not char_dir.is_dir():
        continue
    for img_path in char_dir.glob("*.png"):
        img = Image.open(img_path).convert("L")
        # optional: binarize if your glyphs need thresholding:
        # img = img.point(lambda x: 0 if x<128 else 255, mode='1')
        txt = pytesseract.image_to_string(img, config=TESSERACT_CONFIG)
        pred = txt.strip().upper()
        # take first character only (in case of noise)
        pred = pred[0] if len(pred)>0 else ""
        y_true.append(true_char)
        y_pred.append(pred)
        print(f"Label: {true_char} → Pred: {pred}")
# Compute accuracy
correct = sum(t==p for t,p in zip(y_true, y_pred))
total   = len(y_true)
acc = correct/total
print(f"Character‐level Tesseract Accuracy: {acc*100:5.2f}%  ({correct}/{total})")
# Build & plot a confusion matrix for the most frequent errors
labels = sorted(set(y_true))
cm = np.zeros((len(labels), len(labels)), dtype=int)
idx = {c:i for i,c in enumerate(labels)}
for t,p in zip(y_true, y_pred):
    i, j = idx[t], idx.get(p, None)
    if j is None:
        # treat unknown predictions as a special “?” class
        continue
    cm[i, j] += 1
plt.figure(figsize=(8,6))
sns.heatmap(cm, xticklabels=labels, yticklabels=labels, fmt="d", cmap="Blues")
plt.xlabel("Tesseract Predicted")
plt.ylabel("Ground Truth")
plt.title("Confusion Matrix on Character Test Set")
plt.show()

# Tesseract over Word Dataset
# Configuration: test/ dir
TEST_DIR = "/content/drive/MyDrive/MScProject/data/words3/test"

# Set up Tesseract: only uppercase A–Z, single line (--psm 7)
tess_config = r"--oem 1 --psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ"

# Run OCR over every image, collect predictions & ground truth
gt_labels = []
pred_labels = []

for cls in sorted(os.listdir(TEST_DIR)):
    cls_path = os.path.join(TEST_DIR, cls)
    if not os.path.isdir(cls_path):
        continue
    for img_path in glob.glob(os.path.join(cls_path, "*.png")):
        # ground truth is the folder name
        gt = cls
        # load image as grayscale
        img = Image.open(img_path).convert("L")
        # optional thresholding:
        # img = img.point(lambda x: 0 if x<128 else 255, '1')
        pred = pytesseract.image_to_string(img, config=tess_config)
        pred = pred.strip().upper()

        gt_labels.append(gt)
        pred_labels.append(pred)
        print(f"Label: {gt} → Pred: {pred}")

# Exact-match accuracy
exact_acc = np.mean([p == g for p, g in zip(pred_labels, gt_labels)])
print(f"Exact match accuracy: {exact_acc:.4%}")

# Average character-error rate (CER) and word-error rate (WER)
avg_cer = np.mean([cer(g, p) for p, g in zip(pred_labels, gt_labels)])
avg_wer = np.mean([wer(g, p) for p, g in zip(pred_labels, gt_labels)])
print(f"Mean CER: {avg_cer:.4f}")
print(f"Mean WER: {avg_wer:.4f}")