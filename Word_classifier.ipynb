{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-amy/project-codebase/blob/main/Word_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNvi5n0bLCda"
      },
      "source": [
        "# **CNN Sliding-Window Model for 3-Letter Word De-Obfuscation**\n",
        "Stage 2 of MSc Project — Ashraf Muhammed Yusuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3t0wN17LOBf"
      },
      "source": [
        "# **1. Colab Environment Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U3XkX4M9gxev"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q tensorflow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UUMBUoObLUnb"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import os, sys, random, itertools, pathlib, math, shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf, os, numpy as np\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from tensorflow.keras import mixed_precision\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping, ReduceLROnPlateau)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import defaultdict\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from tqdm import tqdm\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91L4Z6naS9gN"
      },
      "outputs": [],
      "source": [
        "# 1.3 Mount Drive & define base path\n",
        "# Mount Drive so you can read datasets and write checkpoints\n",
        "# Link to dataset:\n",
        "# https://drive.google.com/drive/folders/1sfNG1PkmTPBe1wOSQXZmfdkvR97Hn9lk?usp=sharing\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-w2FvZZOTtyK"
      },
      "outputs": [],
      "source": [
        "# (Optional—but useful) turn on XLA JIT for extra speed\n",
        "tf.config.optimizer.set_jit(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiuZCz5QZ-mr"
      },
      "source": [
        "# **2. Data Generation**\n",
        "This block generates the 'three-letter words' dataset afresh if you do not already have it (You can access it here: https://drive.google.com/drive/folders/1kygA17GiCeCs8qTeDBEndU6TkXnEu-m7?usp=drive_link). It synthesizes three three-letter words from the character dataset (https://drive.google.com/drive/folders/1eUaTNW8zVjTArg0JszbCdCEq0tTdx89n?usp=drive_link)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KgzKPaWRcDTn"
      },
      "outputs": [],
      "source": [
        "# paths & constants\n",
        "BASE_PATH = Path(\"/content/drive/MyDrive/MScProject\")\n",
        "GLYPH_DIR = Path(f\"{BASE_PATH}/data/characters/train\")\n",
        "DATA_ROOT = Path(f\"{BASE_PATH}/data/words3\")\n",
        "CKPT_DIR = f\"{BASE_PATH}/words3_ckpt_best.keras\"\n",
        "BATCH = 128\n",
        "IMG_H = IMG_W = 64\n",
        "IMG_SHAPE = (IMG_H, IMG_W)\n",
        "PATCH_W = IMG_W // 3\n",
        "VARIANTS_PER = 5 # per word\n",
        "EXPECTED_CLASSES = 26**3 # 26³ = 17,576\n",
        "FINAL_TEST_FRAC = 0.20\n",
        "SEED = 42\n",
        "PATCH_W = IMG_W // 3 # 21 when IMG_W = 64\n",
        "\n",
        "random.seed(SEED)\n",
        "train_dir = DATA_ROOT / \"train\"\n",
        "test_dir  = DATA_ROOT / \"test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZDXwT4LYEsr"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 3-Letter Word Dataset Builder  (one-cell, idempotent)\n",
        "# ------------------------------------------------\n",
        "# If words3/train & words3/test already contain 17,576 class folders\n",
        "#   - skips everything (fast guard)\n",
        "# Else:\n",
        "#   1. Generates synthetic uppercase dataset (train/ val/ test/)\n",
        "#   2. Merges val & test into train\n",
        "#   3. Re-splits 80 / 20 into final train/ and test/\n",
        "# ------------------------------------------------\n",
        "# Adjust DATA_ROOT and GLYPH_DIR to your own Drive paths.\n",
        "# ================================================================\n",
        "\n",
        "# fast guard\n",
        "def class_dir_count(p: Path) -> int:\n",
        "  return sum(1 for q in p.iterdir() if q.is_dir())\n",
        "\n",
        "\n",
        "if (train_dir.exists() and test_dir.exists() and\n",
        "  class_dir_count(train_dir) == EXPECTED_CLASSES and\n",
        "  class_dir_count(test_dir)  == EXPECTED_CLASSES):\n",
        "  print(\"words3 dataset already complete – skipping generation.\")\n",
        "  raise SystemExit\n",
        "\n",
        "# glyph reservoir\n",
        "letter_pool = {ltr: glob(str(GLYPH_DIR / ltr / \"*.png\"))\n",
        "               for ltr in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"}\n",
        "missing = [l for l,v in letter_pool.items() if not v]\n",
        "if missing:\n",
        "    sys.exit(f\"Missing glyphs for {missing} in {GLYPH_DIR}\")\n",
        "\n",
        "# obfuscation maps\n",
        "LEET = {\n",
        "  'A': ['Α', '4', 'Д', 'Ä', 'Á', 'À', 'Â', '@', 'Δ'],\n",
        "  'B': ['8', 'β', 'Β', 'В'],\n",
        "  'C': ['Ç', 'Ć', 'Č', 'С'],\n",
        "  'D': ['Ð', 'Ď'],\n",
        "  'E': ['3', 'Σ', 'Έ', 'Ε', 'Е', 'Ë', 'É', 'È', 'Ê'],\n",
        "  'F': ['Φ', 'Ƒ'],\n",
        "  'G': ['6', 'Ğ', 'Ģ', 'Γ'],\n",
        "  'H': ['Η', 'Н'],\n",
        "  'I': ['1', '|', 'Í', 'Ì', 'Î', 'Ï', 'И'],\n",
        "  'J': ['Ј'],\n",
        "  'K': ['Κ', 'К'],\n",
        "  'L': ['Ι', 'Ł', 'Ĺ', 'Л'],\n",
        "  'M': ['Μ', 'М'],\n",
        "  'N': ['Ν', 'Ń', 'Ñ', 'Н'],\n",
        "  'O': ['0', 'Θ', 'Ο', 'Ө', 'Ø', 'Ö', 'Ó', 'Ò', 'Ô'],\n",
        "  'P': ['Ρ', 'Р'],\n",
        "  'Q': ['Φ'],\n",
        "  'R': ['®', 'Я', 'Ř', 'Ŕ'],\n",
        "  'S': ['5', '$', 'Ѕ', 'Ś', 'Š'],\n",
        "  'T': ['Τ', 'Т'],\n",
        "  'U': ['Υ', 'Ц', 'Ü', 'Ú', 'Ù', 'Û'],\n",
        "  'V': ['Ѵ', 'V'],\n",
        "  'W': ['Ω', 'Ѡ', 'Ψ', 'Ш', 'Щ'],\n",
        "  'X': ['Χ', 'Ж', 'Х'],\n",
        "  'Y': ['Υ', 'Ү', 'Ý', 'Ÿ'],\n",
        "  'Z': ['Ζ', 'Ż', 'Ź', 'Ž', 'З', '2']\n",
        "}\n",
        "HOMO = {\n",
        "  'A':'Α',\n",
        "  'B':'Β',\n",
        "  'C':'С',\n",
        "  'E':'Ε',\n",
        "  'H':'Н',\n",
        "  'K':'Κ',\n",
        "  'M':'Μ',\n",
        "  'O':'О',\n",
        "  'P':'Р',\n",
        "  'T':'Τ',\n",
        "  'X':'Χ',\n",
        "  'Y':'Υ',\n",
        "  'Z':'Ζ'\n",
        "}\n",
        "\n",
        "# Obfuscation helper\n",
        "def obfuscate(ch):\n",
        "  mode = random.choices((\"none\",\"leet\",\"homo\"), weights=(0.5,0.4,0.1))[0]\n",
        "  if mode==\"leet\" and ch in LEET: return random.choice(LEET[ch])\n",
        "  if mode==\"homo\" and ch in HOMO: return HOMO[ch]\n",
        "  return ch\n",
        "\n",
        "# image composer\n",
        "def stitch(word, out_path):\n",
        "  canvas = Image.new(\"L\",(IMG_W,IMG_H),color=255)\n",
        "  for i,ch in enumerate(word):\n",
        "    if random.random()<0.7:\n",
        "      glyph = Image.open(random.choice(letter_pool[ch])).resize((PATCH_W,IMG_H))\n",
        "    else:\n",
        "      glyph = Image.new(\"L\",(PATCH_W,IMG_H),color=255)\n",
        "      ImageDraw.Draw(glyph).text((4,4), obfuscate(ch), fill=0)\n",
        "    canvas.paste(glyph,(i*PATCH_W,0))\n",
        "  if random.random()<0.3:\n",
        "    dx = random.randint(-2,2)\n",
        "    canvas = canvas.transform(canvas.size, Image.AFFINE, (1,0,dx,0,1,0))\n",
        "  canvas.save(out_path)\n",
        "\n",
        "# 1. generate initial train/val/test\n",
        "if not (DATA_ROOT/\"train\").exists():\n",
        "  print(\"➤ Generating synthetic dataset …\")\n",
        "  words = [\"\".join(p) for p in itertools.product(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", repeat=3)]\n",
        "  random.shuffle(words)\n",
        "  n = len(words)\n",
        "  i1, i2 = int(0.70*n), int(0.85*n)\n",
        "  splits = {\"train\":words[:i1], \"val\":words[i1:i2], \"test\":words[i2:]}\n",
        "\n",
        "  for split, wl in splits.items():\n",
        "    for w in tqdm(wl, desc=split):\n",
        "      cls_dir = DATA_ROOT/split/w\n",
        "      cls_dir.mkdir(parents=True, exist_ok=True)\n",
        "      for k in range(VARIANTS_PER):\n",
        "        stitch(w, cls_dir/f\"{w}_{k}.png\")\n",
        "\n",
        "# 2. merge val/test into train\n",
        "for old in (\"val\",\"test\"):\n",
        "  p = DATA_ROOT/old\n",
        "  if p.exists():\n",
        "    for cls in tqdm(list(p.iterdir()), desc=f\"Merging {old}\"):\n",
        "      target = DATA_ROOT/\"train\"/cls.name\n",
        "      target.mkdir(parents=True, exist_ok=True)\n",
        "      for img in cls.glob(\"*.png\"):\n",
        "        shutil.move(img, target/img.name)\n",
        "      shutil.rmtree(p)\n",
        "\n",
        "# 3. create final test/ split (80/20)\n",
        "print(\"➤ Splitting 80/20 into final train & test …\")\n",
        "test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for cls in tqdm(list(train_dir.iterdir()), desc=\"80/20\"):\n",
        "  imgs = list(cls.glob(\"*.png\"))\n",
        "  n_move = math.floor(len(imgs)*FINAL_TEST_FRAC)\n",
        "  random.shuffle(imgs)\n",
        "  tgt_cls = test_dir/cls.name\n",
        "  tgt_cls.mkdir(parents=True, exist_ok=True)\n",
        "  for img in imgs[:n_move]:\n",
        "    shutil.move(img, tgt_cls/img.name)\n",
        "\n",
        "# stats\n",
        "print(\"\\nDataset ready.\")\n",
        "print(\"Class folders  :\", class_dir_count(train_dir), \"(train) |\",\n",
        "  class_dir_count(test_dir), \"(test)\")\n",
        "print(\"Image counts   :\", sum(1 for _ in train_dir.rglob(\"*.png\")), \"(train) |\",\n",
        "  sum(1 for _ in test_dir.rglob(\"*.png\")),  \"(test)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuumCMV4NETu"
      },
      "source": [
        "# **3. Load & Freeze the Single-Char Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DidvFD0MNIT5"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.models.load_model(f\"{BASE_PATH}/char_cnn_ckpt_best.keras\")\n",
        "base_model.trainable = False #freeze weights initially\n",
        "\n",
        "print(\"Base model frozen — params:\", base_model.count_params())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4cJg9cNkwe"
      },
      "source": [
        "# **4. Data Loading & Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YNtUaOyNkZr"
      },
      "outputs": [],
      "source": [
        "# Train dataset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_dir,\n",
        "  labels=\"inferred\",\n",
        "  label_mode=\"categorical\",\n",
        "  batch_size=BATCH,\n",
        "  image_size=IMG_SHAPE,\n",
        "  color_mode=\"grayscale\",\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=42\n",
        ")\n",
        "\n",
        "# Val dataset\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  train_dir,\n",
        "  labels=\"inferred\",\n",
        "  label_mode=\"categorical\",\n",
        "  batch_size=BATCH,\n",
        "  image_size=IMG_SHAPE,\n",
        "  color_mode=\"grayscale\",\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=42\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  test_dir,\n",
        "  labels=\"inferred\",\n",
        "  label_mode=\"categorical\",\n",
        "  batch_size=BATCH,\n",
        "  image_size=IMG_SHAPE,\n",
        "  color_mode=\"grayscale\",\n",
        "  shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Visual Sanity Check**"
      ],
      "metadata": {
        "id": "6b3XnHPMxyen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility to display examples from each set\n",
        "def show_examples(ds, ds_name, num=5):\n",
        "  # Take one batch\n",
        "  for images, labels in ds.take(1):\n",
        "    images = images.numpy()\n",
        "    labels = labels.numpy()\n",
        "    class_names = ds.class_names\n",
        "    break\n",
        "\n",
        "  plt.figure(figsize=(6,6))\n",
        "  for i in range(num):\n",
        "    ax = plt.subplot(3, 3, i+1)\n",
        "    img = images[i].squeeze()  # shape: (H,W) since grayscale\n",
        "    lbl = class_names[labels[i].argmax()]\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f\"{ds_name}: {lbl}\")\n",
        "    plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# Display 5 examples from each split\n",
        "show_examples(train_ds, \"Train\")\n",
        "show_examples(val_ds, \"Val\")\n",
        "show_examples(test_ds, \"Test\")"
      ],
      "metadata": {
        "id": "CsR_L_nSsN_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4xIlbVLN-hN"
      },
      "source": [
        "# **6. Build the Sliding-Window Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVuQewrCOAhv"
      },
      "outputs": [],
      "source": [
        "def extract_patch(x, idx):\n",
        "  start = idx * PATCH_W\n",
        "  return x[:, :, start:start+PATCH_W, :] # (None, 64, 21, 1)\n",
        "\n",
        "inputs = tf.keras.Input(shape=(IMG_H, IMG_W, 1))\n",
        "logits = []\n",
        "\n",
        "for i in range(3):\n",
        "  patch = tf.keras.layers.Lambda(lambda z, i=i: extract_patch(z, i))(inputs)\n",
        "  patch = tf.keras.layers.Resizing(IMG_H, IMG_H)(patch) # -> (64 x 64 x 1)\n",
        "  # Re-use frozen base_model (shared weights)\n",
        "  logits.append(base_model(patch)) # (None, 26)\n",
        "\n",
        "concat = tf.keras.layers.Concatenate()(logits) # (None, 78)\n",
        "# Hidden layer #1\n",
        "h1 = tf.keras.layers.Dense(256, activation='relu')(concat)\n",
        "h1 = tf.keras.layers.BatchNormalization()(h1)\n",
        "h1 = tf.keras.layers.Dropout(0.4)(h1)\n",
        "# Hidden layer #2\n",
        "h1 = tf.keras.layers.Dense(256, activation='relu')(h1)\n",
        "h1 = tf.keras.layers.Dropout(0.4)(h1)\n",
        "outputs = tf.keras.layers.Dense(EXPECTED_CLASSES, activation='softmax')(h1)\n",
        "\n",
        "word_model = tf.keras.Model(inputs, outputs)\n",
        "word_model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "word_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWVbhlvKOMjg"
      },
      "source": [
        "# **7. Callbacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ifsAp6O1OOrx"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "  # 1. Checkpoint\n",
        "  ModelCheckpoint(CKPT_DIR, save_best_only=True, monitor='val_loss'),\n",
        "  # 2. Early stopping\n",
        "  EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
        "  # 3. Learning rate scheduler\n",
        "  ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPi4tnFGOS4R"
      },
      "source": [
        "# **8. Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aai8C7KOVVh"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = word_model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=20,\n",
        "  callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional: if accuracy is not great)\n",
        "# Freeze all weights except the last N blocks\n",
        "N = 1\n",
        "# Un-freeze last 3 layers of base_model\n",
        "for layer in base_model.layers[-N:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "# Re-compile with lower LR\n",
        "word_model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "ft_history = word_model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  initial_epoch=history.epoch[-1] + 1,\n",
        "  epochs=history.epoch[-1] + 5,\n",
        "  callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "yOKfnAj92MqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHMjuGLOOj5H"
      },
      "source": [
        "# **9. Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdfVNFw1OlRs"
      },
      "outputs": [],
      "source": [
        "word_model = tf.keras.models.load_model(CKPT_DIR) # best checkpoint\n",
        "test_loss, test_acc = word_model.evaluate(test_ds)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Util for plotting confusion matrix\n",
        "def plot_confusion_matrix(cm, class_names, title=\"Confusion Matrix\"):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      cm (np.ndarray): square confusion matrix\n",
        "      class_names (List[str]): labels in the same order used to build cm\n",
        "  \"\"\"\n",
        "  fig, ax = plt.subplots(figsize=(10, 9))\n",
        "  im = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "  ax.figure.colorbar(im, ax=ax, fraction=0.045)\n",
        "\n",
        "  # axes & ticks\n",
        "  ax.set(\n",
        "    xticks=np.arange(len(class_names)),\n",
        "    yticks=np.arange(len(class_names)),\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    ylabel=\"True label\",\n",
        "    xlabel=\"Predicted label\",\n",
        "    title=title,\n",
        "  )\n",
        "  plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\", va=\"center\")\n",
        "\n",
        "  # annotate cells\n",
        "  thresh = cm.max() / 2.0\n",
        "  for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "      ax.text(\n",
        "        j, i, format(cm[i, j], \"d\"),\n",
        "        ha=\"center\", va=\"center\",\n",
        "        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "        fontsize=8\n",
        "      )\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# Classification report\n",
        "y_pred, y_true = [], []\n",
        "for x, y in test_ds:\n",
        "  y_pred.extend(np.argmax(word_model.predict(x), axis=1))\n",
        "  y_true.extend(np.argmax(y.numpy(), axis=1))\n",
        "print(classification_report(y_true, y_pred, target_names=train_ds.class_names))\n",
        "\n",
        "# Confusion matrix heat-map (optional)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plot_confusion_matrix(cm, train_ds.class_names, title=\"3-Letter Word Confusion Matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeZuj2jpOrqf"
      },
      "source": [
        "# **10. Qualitative Error Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSwpu5vjOuEs"
      },
      "outputs": [],
      "source": [
        "# Plot a few misclassified 3-letter words\n",
        "mis_idx = [i for i,(t,p) in enumerate(zip(y_true, y_pred)) if t != p]\n",
        "show_examples(test_ds.unbatch().skip(mis_idx[0]), \"Misclassified example\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyMSjoP5lq0unyXTfqcBn4Ms",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}