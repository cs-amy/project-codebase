{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-amy/project-codebase/blob/main/notebooks/colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nog1hADaZOK"
      },
      "source": [
        "# MSc Project Model Training on Google Colab\n",
        "\n",
        "This notebook sets up the environment for training the letter classification model on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdOUGOlnaZOM"
      },
      "source": [
        "## 1. Clone the GitHub Repository\n",
        "\n",
        "First, clone your GitHub repository. Replace `YOUR_GITHUB_USERNAME` and `YOUR_REPO_NAME` with your actual GitHub username and repository name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-0ls620aZOM"
      },
      "outputs": [],
      "source": [
        "# Delete the project-codebase directory if it exists\n",
        "!rm -rf project-codebase\n",
        "\n",
        "!git clone https://github.com/cs-amy/project-codebase.git\n",
        "%cd project-codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t8XlWpLaZOM"
      },
      "source": [
        "## 2. Mount Google Drive (for data files)\n",
        "\n",
        "If the project's data files are stored in Google Drive, mount it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIaBlVzOaZON"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create symbolic links to the data directory\n",
        "!ln -s /content/drive/MyDrive/MScProject/data data"
      ],
      "metadata": {
        "id": "EXHAQINrBS-o"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9Pq7TuaZON"
      },
      "source": [
        "## 3. Install Dependencies\n",
        "\n",
        "Install the required packages from the requirements.txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReSHp1RgaZON"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "\n",
        "# Specific versions of PyTorch with CUDA support\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cStl18DfaZON"
      },
      "source": [
        "## 4. Set Up Python Path\n",
        "\n",
        "Ensure that the project modules can be imported correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34qpQFtlaZON"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/project-codebase')\n",
        "\n",
        "# Verify imports work\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import torch\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from rich.console import Console\n",
        "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "from rich.live import Live\n",
        "from rich.layout import Layout\n",
        "\n",
        "from src.data.data_loader import DataLoader\n",
        "from src.models.letter_classifier import LetterClassifierCNN\n",
        "from src.train.trainer import ModelTrainer\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB46ZRFEaZON"
      },
      "source": [
        "## 5. Load Configuration\n",
        "\n",
        "Load the training configuration from the config file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLaLqnQBaZON"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open('configs/train_config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Configuration loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils.config import load_config, get_model_config, get_training_config, get_data_config\n",
        "from src.models.letter_classifier import get_model\n",
        "from src.train.trainer import ModelTrainer"
      ],
      "metadata": {
        "id": "jHpbOdZE6__q"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import logging\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def validate_data_directory(data_dir: Path) -> None:\n",
        "    \"\"\"\n",
        "    Validate the data directory structure.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Path to the data/characters directory\n",
        "    \"\"\"\n",
        "    required_dirs = [\n",
        "        \"regular/train\",\n",
        "        \"regular/test\",\n",
        "        \"obfuscated/train\",\n",
        "        \"obfuscated/test\"\n",
        "    ]\n",
        "\n",
        "    missing_dirs = []\n",
        "    for dir_path in required_dirs:\n",
        "        full_path = data_dir / dir_path\n",
        "        if not full_path.exists():\n",
        "            missing_dirs.append(dir_path)\n",
        "\n",
        "    if missing_dirs:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing required directories in {data_dir}:\\n\" +\n",
        "            \"\\n\".join(f\"- {d}\" for d in missing_dirs)\n",
        "        )\n",
        "\n",
        "    # Log directory structure\n",
        "    logger.info(\"Data directory structure validated:\")\n",
        "    logger.info(f\"Root: {data_dir}\")\n",
        "    for dir_path in required_dirs:\n",
        "        full_path = data_dir / dir_path\n",
        "        num_files = len(list(full_path.glob(\"**/*.png\")))\n",
        "        logger.info(f\"- {dir_path}: {num_files} PNG files\")\n",
        "\n",
        "\n",
        "class CharacterDataset(Dataset):\n",
        "    \"\"\"Dataset for character images (regular or obfuscated).\"\"\"\n",
        "\n",
        "    # Class-level character mapping\n",
        "    CHAR_TO_IDX = {chr(97 + i): i for i in range(26)}  # a-z to 0-25\n",
        "    IDX_TO_CHAR = {i: chr(97 + i) for i in range(26)}  # 0-25 to a-z\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str | Path,\n",
        "        image_size: Tuple[int, int] = (28, 28),\n",
        "        transform: Optional[transforms.Compose] = None,\n",
        "        is_training: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            data_dir: Directory containing character images (e.g., data/characters/regular/train)\n",
        "            image_size: Target size for images (height, width)\n",
        "            transform: Optional additional transformations\n",
        "            is_training: Whether this is a training dataset\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        if not self.data_dir.exists():\n",
        "            raise FileNotFoundError(f\"Data directory not found: {self.data_dir}\")\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.is_training = is_training\n",
        "\n",
        "        # Get all character directories (a-z)\n",
        "        self.char_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])\n",
        "        if not self.char_dirs:\n",
        "            raise ValueError(f\"No character directories found in {self.data_dir}\")\n",
        "\n",
        "        # Get all image paths and labels\n",
        "        self.images, self.labels = self._load_dataset()\n",
        "\n",
        "        # Set up transformations\n",
        "        self.transform = transform if transform is not None else self._get_default_transforms()\n",
        "\n",
        "        # Log dataset statistics\n",
        "        logger.info(f\"Loaded {len(self.images)} images from {self.data_dir}\")\n",
        "        char_counts = {char: sum(1 for l in self.labels if l == idx)\n",
        "                       for char, idx in self.CHAR_TO_IDX.items()}\n",
        "        logger.info(\"Character distribution:\")\n",
        "        for char, count in char_counts.items():\n",
        "            logger.info(f\"- {char}: {count} images\")\n",
        "\n",
        "    def _load_dataset(self) -> Tuple[List[Path], List[int]]:\n",
        "        \"\"\"Load all image paths and their corresponding labels.\"\"\"\n",
        "        images, labels = [], []\n",
        "\n",
        "        for char_dir in self.char_dirs:\n",
        "            char = char_dir.name.lower()\n",
        "            if char not in self.CHAR_TO_IDX:\n",
        "                logger.warning(f\"Skipping unknown character directory: {char}\")\n",
        "                continue\n",
        "\n",
        "            label = self.CHAR_TO_IDX[char]\n",
        "\n",
        "            # Get all PNG images in this directory\n",
        "            char_images = list(char_dir.glob(\"*.png\"))\n",
        "            images.extend(char_images)\n",
        "            labels.extend([label] * len(char_images))\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def _get_default_transforms(self) -> transforms.Compose:\n",
        "        \"\"\"Get default transformation pipeline.\"\"\"\n",
        "        transform_list = [\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.Grayscale(1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ]\n",
        "\n",
        "        if self.is_training:\n",
        "            transform_list.insert(1, transforms.RandomRotation(10))\n",
        "            transform_list.insert(2, transforms.RandomAffine(\n",
        "                degrees=0,\n",
        "                translate=(0.1, 0.1),\n",
        "                scale=(0.9, 1.1)\n",
        "            ))\n",
        "\n",
        "        return transforms.Compose(transform_list)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        \"\"\"Get a single sample.\"\"\"\n",
        "        image_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load and convert image\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "            image = self.transform(image)\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading image {image_path}: {e}\")\n",
        "            # Return a blank image and the label if there's an error\n",
        "            return torch.zeros((1, *self.image_size)), label\n",
        "\n",
        "    @classmethod\n",
        "    def get_char_mapping(cls) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "        \"\"\"\n",
        "        Get the character to index and index to character mappings.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (char_to_idx, idx_to_char) dictionaries\n",
        "        \"\"\"\n",
        "        return cls.CHAR_TO_IDX, cls.IDX_TO_CHAR\n",
        "\n",
        "\n",
        "def get_data_loaders(\n",
        "    data_dir: str | Path,\n",
        "    batch_size: int = 32,\n",
        "    image_size: Tuple[int, int] = (28, 28),\n",
        "    num_workers: int = 4,\n",
        "    augment: bool = True\n",
        ") -> Dict[str, DataLoader]:\n",
        "    \"\"\"\n",
        "    Create data loaders for training, validation, and testing.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Path to the data/characters directory (e.g., data/characters)\n",
        "        batch_size: Batch size for training\n",
        "        image_size: Target size for images\n",
        "        num_workers: Number of worker processes for data loading\n",
        "        augment: Whether to use data augmentation (applied in training mode)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing train, val, and test data loaders.\n",
        "    \"\"\"\n",
        "    data_dir = Path(data_dir)\n",
        "\n",
        "    # Validate directory structure\n",
        "    validate_data_directory(data_dir)\n",
        "\n",
        "    # Load training datasets for both regular and obfuscated characters\n",
        "    train_regular = CharacterDataset(\n",
        "        data_dir / \"regular\" / \"train\",\n",
        "        image_size=image_size,\n",
        "        is_training=True\n",
        "    )\n",
        "    train_obfuscated = CharacterDataset(\n",
        "        data_dir / \"obfuscated\" / \"train\",\n",
        "        image_size=image_size,\n",
        "        is_training=True\n",
        "    )\n",
        "\n",
        "    # Load test datasets for both regular and obfuscated characters\n",
        "    test_regular = CharacterDataset(\n",
        "        data_dir / \"regular\" / \"test\",\n",
        "        image_size=image_size,\n",
        "        is_training=False\n",
        "    )\n",
        "    test_obfuscated = CharacterDataset(\n",
        "        data_dir / \"obfuscated\" / \"test\",\n",
        "        image_size=image_size,\n",
        "        is_training=False\n",
        "    )\n",
        "\n",
        "    # Combine datasets for training and testing\n",
        "    train_dataset = ConcatDataset([train_regular, train_obfuscated])\n",
        "    test_dataset = ConcatDataset([test_regular, test_obfuscated])\n",
        "\n",
        "    # Create train/validation split (80/20) on the training dataset\n",
        "    train_length = int(0.8 * len(train_dataset))\n",
        "    val_length = len(train_dataset) - train_length\n",
        "    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_length, val_length])\n",
        "\n",
        "    # Log dataset statistics\n",
        "    logger.info(\"Combined dataset statistics:\")\n",
        "    logger.info(f\"- Training set (after split): {len(train_subset)} images\")\n",
        "    logger.info(f\"- Validation set: {len(val_subset)} images\")\n",
        "    logger.info(f\"- Test set: {len(test_dataset)} images\")\n",
        "\n",
        "    # Create data loaders for each split\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"train\": train_loader,\n",
        "        \"val\": val_loader,\n",
        "        \"test\": test_loader\n",
        "    }"
      ],
      "metadata": {
        "id": "qgLaZq3rF296"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7YtmyjWaZON"
      },
      "source": [
        "## 6. Train Model\n",
        "\n",
        "Now you can run the training code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mNsc1BZUaZON"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize rich console\n",
        "console = Console()\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"\n",
        "    Set up the training device (GPU/CPU) based on availability.\n",
        "    Uses MPS for Apple Silicon, CUDA for NVIDIA GPUs, or falls back to CPU.\n",
        "    \"\"\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        console.print(\"[green]GPU available: Using Metal Performance Shaders (MPS)[/green]\")\n",
        "    elif torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        console.print(\"[green]GPU available: Using CUDA[/green]\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        console.print(\"[yellow]No GPU detected: Using CPU[/yellow]\")\n",
        "    return device\n",
        "\n",
        "def get_optimal_batch_size(image_size, available_memory_gb=None):\n",
        "    \"\"\"\n",
        "    Calculate optimal batch size based on available memory and image size.\n",
        "\n",
        "    Args:\n",
        "        image_size (tuple): Image dimensions (height, width)\n",
        "        available_memory_gb (float): Available GPU memory in GB. If None, estimates based on system.\n",
        "\n",
        "    Returns:\n",
        "        int: Optimal batch size\n",
        "    \"\"\"\n",
        "    # Estimate memory if not provided\n",
        "    if available_memory_gb is None:\n",
        "        if torch.backends.mps.is_available() or torch.cuda.is_available():\n",
        "            available_memory_gb = 16  # Conservative estimate for GPU memory\n",
        "        else:\n",
        "            available_memory_gb = 8   # Conservative estimate for CPU memory\n",
        "\n",
        "    # Calculate memory requirements per sample\n",
        "    bytes_per_pixel = 4  # float32\n",
        "    sample_memory = image_size[0] * image_size[1] * bytes_per_pixel\n",
        "\n",
        "    # Reserve 20% of memory for the model and other operations\n",
        "    usable_memory = available_memory_gb * 1e9 * 0.2\n",
        "\n",
        "    # Calculate batch size\n",
        "    optimal_batch_size = min(128, int(usable_memory / sample_memory))\n",
        "\n",
        "    # Ensure batch size is at least 16\n",
        "    return max(16, optimal_batch_size)\n",
        "\n",
        "def resume_training(trainer, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Resume training from a checkpoint if available.\n",
        "\n",
        "    Args:\n",
        "        trainer (Trainer): Training instance\n",
        "        checkpoint_path (Path): Path to checkpoint file\n",
        "    \"\"\"\n",
        "    if checkpoint_path.exists():\n",
        "        trainer.load_checkpoint(checkpoint_path)\n",
        "        console.print(f\"[green]Resumed training from {checkpoint_path}[/green]\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def monitor_memory():\n",
        "    \"\"\"\n",
        "    Monitor and log GPU/CPU memory usage during training.\n",
        "    Returns a formatted string with memory information.\n",
        "    \"\"\"\n",
        "    memory_info = []\n",
        "\n",
        "    if torch.backends.mps.is_available():\n",
        "        try:\n",
        "            used_memory = torch.mps.current_allocated_memory() / 1e9\n",
        "            memory_info.append(f\"GPU Memory Used: {used_memory:.2f} GB\")\n",
        "        except:\n",
        "            memory_info.append(\"GPU Memory: Not available\")\n",
        "    elif torch.cuda.is_available():\n",
        "        try:\n",
        "            used_memory = torch.cuda.memory_allocated() / 1e9\n",
        "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "            memory_info.append(f\"GPU Memory: {used_memory:.2f}GB / {total_memory:.2f}GB\")\n",
        "        except:\n",
        "            memory_info.append(\"GPU Memory: Not available\")\n",
        "    else:\n",
        "        import psutil\n",
        "        process = psutil.Process()\n",
        "        used_memory = process.memory_info().rss / 1e9\n",
        "        total_memory = psutil.virtual_memory().total / 1e9\n",
        "        memory_info.append(f\"CPU Memory: {used_memory:.2f}GB / {total_memory:.2f}GB\")\n",
        "\n",
        "    return \" | \".join(memory_info)\n",
        "\n",
        "def create_layout():\n",
        "    \"\"\"Create the layout for the training display.\"\"\"\n",
        "    layout = Layout()\n",
        "    layout.split(\n",
        "        Layout(name=\"header\", size=3),\n",
        "        Layout(name=\"body\"),\n",
        "        Layout(name=\"footer\", size=3)\n",
        "    )\n",
        "    return layout\n",
        "\n",
        "def create_header():\n",
        "    \"\"\"Create the header panel with training information.\"\"\"\n",
        "    return Panel(\n",
        "        \"[bold blue]Letter Classification Model Training[/bold blue]\",\n",
        "        style=\"white on blue\"\n",
        "    )\n",
        "\n",
        "def create_footer(epoch, total_epochs, train_loss, train_acc, val_loss, val_acc, lr):\n",
        "    \"\"\"Create the footer panel with current training metrics.\"\"\"\n",
        "    return Panel(\n",
        "        f\"Epoch: {epoch}/{total_epochs} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Train Acc: {train_acc:.2f}% | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Val Acc: {val_acc:.2f}% | \"\n",
        "        f\"LR: {lr:.6f}\",\n",
        "        style=\"white on blue\"\n",
        "    )\n",
        "\n",
        "def create_metrics_table(train_loss, train_acc, val_loss, val_acc):\n",
        "    \"\"\"Create a table with training metrics.\"\"\"\n",
        "    table = Table(title=\"Training Metrics\")\n",
        "    table.add_column(\"Metric\", style=\"cyan\")\n",
        "    table.add_column(\"Value\", style=\"green\")\n",
        "\n",
        "    table.add_row(\"Training Loss\", f\"{train_loss:.4f}\")\n",
        "    table.add_row(\"Training Accuracy\", f\"{train_acc:.2f}%\")\n",
        "    table.add_row(\"Validation Loss\", f\"{val_loss:.4f}\")\n",
        "    table.add_row(\"Validation Accuracy\", f\"{val_acc:.2f}%\")\n",
        "\n",
        "    return table\n",
        "\n",
        "def train():\n",
        "    \"\"\"Main function for training the model.\"\"\"\n",
        "    # Load config\n",
        "    config_path = Path(\"configs/train_config.yaml\")\n",
        "    if not config_path.exists():\n",
        "        console.print(f\"[red]Config file not found: {config_path}[/red]\")\n",
        "        return\n",
        "\n",
        "    config = load_config(config_path)\n",
        "    model_config = get_model_config(config)\n",
        "    training_config = get_training_config(config)\n",
        "    data_config = get_data_config(config)\n",
        "\n",
        "    # Create output directory with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = Path(\"outputs/letter_classifier\") / timestamp\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save config\n",
        "    with open(output_dir / \"config.yaml\", \"w\") as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "    # Calculate optimal batch size\n",
        "    optimal_batch_size = get_optimal_batch_size(model_config[\"input_shape\"][:2])\n",
        "    if optimal_batch_size != training_config[\"batch_size\"]:\n",
        "        console.print(f\"[yellow]Adjusting batch size from {training_config['batch_size']} to {optimal_batch_size} based on available memory[/yellow]\")\n",
        "        training_config[\"batch_size\"] = optimal_batch_size\n",
        "\n",
        "    # Create data loaders\n",
        "    console.print(\"\\n[bold cyan]Loading datasets...[/bold cyan]\")\n",
        "    data_loaders = get_data_loaders(\n",
        "        data_dir=\"/content/drive/MyDrive/MScProject/data/characters\",\n",
        "        batch_size=training_config[\"batch_size\"],\n",
        "        image_size=model_config[\"input_shape\"][:2],\n",
        "        num_workers=4,\n",
        "        augment=data_config.get(\"augmentation\", {}).get(\"use\", True)\n",
        "    )\n",
        "\n",
        "    # Print dataset statistics\n",
        "    train_size = len(data_loaders[\"train\"].dataset)\n",
        "    val_size = len(data_loaders[\"val\"].dataset)\n",
        "    console.print(f\"\\n[green]Dataset Statistics:[/green]\")\n",
        "    console.print(f\"- Training set: {train_size:,} images\")\n",
        "    console.print(f\"- Validation set: {val_size:,} images\")\n",
        "    console.print(f\"- Batch size: {training_config['batch_size']}\")\n",
        "\n",
        "    # Create model\n",
        "    console.print(\"\\n[bold cyan]Initializing model...[/bold cyan]\")\n",
        "    model = get_model(model_config[\"architecture\"], model_config)\n",
        "    console.print(f\"- Input shape: {model_config['input_shape']}\")\n",
        "    console.print(f\"- Number of classes: {model_config['num_classes']}\")\n",
        "    console.print(f\"- Model architecture: {model_config['architecture']}\")\n",
        "\n",
        "    # Set up device\n",
        "    device = setup_device()\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = ModelTrainer(\n",
        "        model=model,\n",
        "        train_loader=data_loaders[\"train\"],\n",
        "        val_loader=data_loaders[\"val\"],\n",
        "        config=training_config,\n",
        "        output_dir=output_dir,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Try to resume from checkpoint\n",
        "    checkpoint_path = output_dir / \"latest_checkpoint.pth\"\n",
        "    if resume_training(trainer, checkpoint_path):\n",
        "        console.print(\"[green]Successfully resumed training from checkpoint[/green]\")\n",
        "\n",
        "    # Create layout for training display\n",
        "    layout = create_layout()\n",
        "\n",
        "    # Start training with rich display\n",
        "    console.print(\"\\n[bold cyan]Starting training...[/bold cyan]\")\n",
        "    with Live(layout, refresh_per_second=4) as live:\n",
        "        for epoch in range(1, training_config[\"epochs\"] + 1):\n",
        "            # Update header\n",
        "            layout[\"header\"].update(create_header())\n",
        "\n",
        "            # Monitor and display memory usage\n",
        "            memory_status = monitor_memory()\n",
        "            console.print(f\"\\n[cyan]Memory Status: {memory_status}[/cyan]\")\n",
        "\n",
        "            # Train for one epoch\n",
        "            train_loss, train_acc = trainer.train_epoch()\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_acc, predictions, targets = trainer.validate()\n",
        "\n",
        "            # Update footer with current metrics\n",
        "            layout[\"footer\"].update(create_footer(\n",
        "                epoch, training_config[\"epochs\"],\n",
        "                train_loss, train_acc,\n",
        "                val_loss, val_acc,\n",
        "                trainer.optimizer.param_groups[0]['lr']\n",
        "            ))\n",
        "\n",
        "            # Update metrics table\n",
        "            layout[\"body\"].update(create_metrics_table(\n",
        "                train_loss, train_acc,\n",
        "                val_loss, val_acc\n",
        "            ))\n",
        "\n",
        "            # Save checkpoint and visualizations\n",
        "            if epoch % 5 == 0:\n",
        "                trainer.save_checkpoint(epoch)\n",
        "                # Monitor memory after checkpoint save\n",
        "                memory_status = monitor_memory()\n",
        "                console.print(f\"[cyan]Memory Status after checkpoint: {memory_status}[/cyan]\")\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                trainer.plot_confusion_matrix(predictions, targets, epoch)\n",
        "\n",
        "    # Save final model and plots\n",
        "    trainer.save_model(\"final\")\n",
        "    trainer.plot_history()\n",
        "\n",
        "    # Final memory status\n",
        "    memory_status = monitor_memory()\n",
        "    console.print(f\"\\n[cyan]Final Memory Status: {memory_status}[/cyan]\")\n",
        "\n",
        "    console.print(\"\\n[bold green]Training completed![/bold green]\")\n",
        "    console.print(f\"Results saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "train()"
      ],
      "metadata": {
        "id": "txynyz8j8VX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDqXoZpYaZON"
      },
      "source": [
        "## 7. Save Results to Google Drive\n",
        "\n",
        "Save the trained model and results to Google Drive for persistence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQmg7BV_aZON"
      },
      "outputs": [],
      "source": [
        "# Create directory for results if it doesn't exist\n",
        "!mkdir -p /content/drive/MyDrive/MScProject/results\n",
        "\n",
        "# Copy results to Google Drive\n",
        "!cp -r results/* /content/drive/MyDrive/MScProject/results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MSc Project Training",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}